{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5 - Ishaan Sathaye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write Spark programs in Scala to solve the following problems. Do not create any data structures, such as hash maps or tree. Use functional, rather than empirical, programming style. That is, do not use  traditional **for**, **while**, or **do-while** statements. Using **foreach** to print the result is fine.\n",
    "\n",
    "1. The program reads a file full of integers and computes the number of times each integer that is divisible by 3 occurs.\n",
    "\n",
    "Example input file:\n",
    "```\n",
    "1 3 10 3\n",
    "6 6 9 6\n",
    "```\n",
    "Example output:\n",
    "```\n",
    "3 appears 2 times, 6 appears 3 times, 9 appears 1 time\n",
    "```\n",
    "\n",
    "Do not worry about sorting the result.\n",
    "\n",
    "Solution:\n",
    "```scala\n",
    "import scala.io._\n",
    "\n",
    "object q1 {\n",
    "    def main(args: Array[String]): Unit = {\n",
    "        Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "        Logger.getLogger(\"akka\").setLevel(Level.OFF)\n",
    "\n",
    "        val conf = new SparkConf().setAppName(\"assignment5\")\n",
    "        val sc = new SparkContext(conf)\n",
    "\n",
    "        val lines = sc.textFile(\"input.txt\").persist()\n",
    "        val numbers = lines.flatMap(_.split(\" \")).map(_.toInt)\n",
    "        val divisibleBy3 = numbers.filter(_ % 3 == 0)\n",
    "        val counts = divisibleBy3.map((_, 1)).reduceByKey(_ + _)\n",
    "        val output = counts.collect()\n",
    "        val res = output.map { case (number, count) => \n",
    "            s\"$number appears $count times\" }.mkString(\", \")\n",
    "        println(res)\n",
    "\n",
    "        sc.stop()\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "2. The program reads a file with employees and a file with departments. The program should print the employee name and department name for each employee. Use a cartesian product and filter to solve the problem. Note that your filter can have the syntax: `.filter{ case (emp, dep) => ... }`. This means that if the input is a tuple of two values, you can process each value individually. Similarly, you can use `.map{ case (emp, dep) => ... }` to format the output.\n",
    "\n",
    "Example Employee file:\n",
    "```\n",
    "John, 23 // John works in department 23\n",
    "Bob, 55\n",
    "Steward, 20\n",
    "Elizabeth, 44\n",
    "```\n",
    "\n",
    "Example Department file:\n",
    "```\n",
    "23, Computer Science\n",
    "20, Mechanical Engineering\n",
    "44, Industrial Engineering\n",
    "55, Electrical Engineering\n",
    "```\n",
    "\n",
    "Example Output:\n",
    "```\n",
    "John, Computer Science\n",
    "Steward, Mechanical Engineering\n",
    "Bob, Electrical Engineering\n",
    "Elizabeth, Industrial Engineering\n",
    "```\n",
    "\n",
    "Solution:\n",
    "```scala\n",
    "import scala.io._\n",
    "\n",
    "object q2 {\n",
    "    def main(args: Array[String]): Unit = {\n",
    "        Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "        Logger.getLogger(\"akka\").setLevel(Level.OFF)\n",
    "\n",
    "        val conf = new SparkConf().setAppName(\"assignment5\")\n",
    "        val sc = new SparkContext(conf)\n",
    "\n",
    "        val employees = sc.textFile(\"employees.txt\")\n",
    "            .map(_.split(\", \"))\n",
    "            .map { case Array(name, dep) => \n",
    "                (name, dep.toInt) }.persist()\n",
    "        \n",
    "        val departments = sc.textFile(\"departments.txt\")\n",
    "            .map(_.split(\", \"))\n",
    "            .map { case Array(dep, name) => \n",
    "                (dep.toInt, name) }.persist()\n",
    "        \n",
    "        val res = employees.cartesian(departments)\n",
    "            .filter { case (emp, dep) => emp._2 == dep._1 }\n",
    "            .map { case (emp, dep) => \n",
    "                s\"${emp._1}, ${dep._2}\" }.collect()\n",
    "        \n",
    "        res.foreach(println)\n",
    "\n",
    "        sc.stop()\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "3. Consider the following input file:\n",
    "```\n",
    "Bob Wilson, 11, B CS201\n",
    "John Back, 23, A CSC369, B CSC366\n",
    "```\n",
    "\n",
    "It contains the student name, student ID, and a list of courses the student has taken (grade and course). Write a program with the following example output:\n",
    "```\n",
    "Bob Wilson, 11, 3.0\n",
    "John Back, 23, 3.5\n",
    "```\n",
    "\n",
    "The program prints the student name, student ID, and their GPA. You can assume there is a single entry per student in the input file. Not that you can use `line.split(\", \", 3)(2).trim()` to only consider the first two commas of a string and get everything after the second comma. Feel free to create a separate method that computes the GPA and a map that converts a letter grade to a numeric value.\n",
    "\n",
    "Use the **aggregate** operator to solve the problem.\n",
    "\n",
    "Solution:\n",
    "```scala\n",
    "import scala.io._\n",
    "\n",
    "object q3 {\n",
    "    def main(args: Array[String]): Unit = {\n",
    "        Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "        Logger.getLogger(\"akka\").setLevel(Level.OFF)\n",
    "\n",
    "        val conf = new SparkConf().setAppName(\"assignment5\")\n",
    "        val sc = new SparkContext(conf)\n",
    "\n",
    "        val lines = sc.textFile(\"input.txt\").persist()\n",
    "        val gpaMap = Map(\"A\" -> 4.0, \"B\" -> 3.0, \"C\" -> 2.0, \"D\" -> 1.0, \"F\" -> 0.0)\n",
    "\n",
    "        val res = lines.map { line =>\n",
    "            val parts = line.split(\", \", 3)\n",
    "            val name = parts(0)\n",
    "            val id = parts(1)\n",
    "            val courses = parts(2).split(\", \").map(_.trim)\n",
    "\n",
    "            val (total, numCourses) = courses.aggregate(0.0, 0)(\n",
    "                { case ((sum, count), course) =>\n",
    "                    (sum + gpaMap(course.split(\" \")(0)), count + 1)}, \n",
    "                { case ((sum1, count1), (sum2, count2)) =>\n",
    "                    (sum1 + sum2, count1 + count2)}) \n",
    "\n",
    "            val gpa = total / numCourses\n",
    "\n",
    "            s\"$name, $id, $gpa\"\n",
    "        }\n",
    "\n",
    "        res.collect().foreach(println)\n",
    "\n",
    "        sc.stop()\n",
    "    }\n",
    "}\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
